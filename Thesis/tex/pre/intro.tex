\chapter*{Introduction} \phantomsection\addcontentsline{toc}{chapter}{Introduction}

Modern robotics has experienced tremendous growth since its inception during the industrial revolution. Whether it is to assist humans in their works, to automate some tasks or to perform dangerous tasks, robots are emerging in a wide variety of applications. Recent technology such as the self-driving car project from Google and the BigDog quadruped robot from Boston Dynamics are feat of engineering that show the evolution of robotics from the not too distant past, when robots could only achieved simple tasks in controlled environments. One of the key elements that allowed for such progress, is the ability of the robot to adapt to changing environments. This is especially true for mobile robotics, where not only the surrounding environment can change, but the robot itself can move and must therefore be able to locate itself. This capability highly depend on algorithms that convert the raw inputs of available sensors into a convenient representation or abstraction of the environment; concept known as artificial perception.

A wide range of sensors are available to assist robots navigation. It is possible, for instance, to estimate the relative movements of the robot using wheel encoders. On the other hand, when the ground friction coefficient is variable or the load-gearing surface is very uneven, pose estimation relying on solely on this sensor is highly unreliable. Similarly, \gls*{imu} are composed of accelerometers and gyroscopes with which it is possible to infer changes in position. Although these sensors can be really precise over short distances, they accumulate error over time, which inevitably leads to a drift on the pose estimation.

The natural solution to this problem is to use sensors capable of providing absolute positioning. Arguably the most popular sensor providing such information is the \gls*{gps}. Then again, there are inherent problems with this sensor. \gls*{gps} requires to receive the signal of at least three satellites at all time. These signals can be blocked by building, terrain, dense foliage or other structures, thus causing important positioning errors or possibly no positioning at all. Alternatively, one can rely on a global map of landmarks representing the environment in which the robot operates. The map can be produced beforehand or gradually when the robot explores the environment, but landmarks will depend on the data acquired by the sensor used. \todo{Here I am, rework last sentence. Might want to end the paragraph here.}

Tactile sensor (vibration data), infrared range finder, \gls*{sonar}, \gls*{radar} (for geometric data) and multiple type of cameras (for appearance data) are examples of such sensors. \todo{could use the word signature, like geometic signature or infrared signature}

\todo{This paragraph have to be changed almost completely. For the remainder of the intro, check Phil recommendations}
Arguably, the most widely used of these sensors is the \gls*{rgb} camera. In addition to its price generally very low, cameras provides a rich data set on the scene in which the robot operates. Despite these obvious advantages, processing camera data generally rely on very complex algorithms that are often based on strong assumptions. The processing power and time required by those algorithms is not always available. The \gls*{lidar} is another sensor that provide rich data about the robot scene. It use laser to measure distances at different angles from the sensor center. Most \gls*{lidar}s provide \gls*{2d} set of points and can be mounted on a rotating unit to create \gls*{3d} data, while other can directly provide such data. \gls*{lidar}s are generally more expensive than cameras, but the geometrical information obtained therewith is often complementary to the appearance information provided by the cameras. For instance, a \gls*{lidar} will faithfully report the flat structure of a white wall, while the lack of visual features will make it impossible for the camera to infer such information. On the other hand, a camera would be able to locate itself using the rich appearance information of a poster on a wall, but the geometrical information of the wall retrieved by the \gls*{lidar} is of little help for the localisation task. Another important difference between those sensors is that the \gls*{lidar} is an active sensor that can be used day and night and which is mostly unaffected by lightning condition, while the camera is a passive sensor for which the compensation for changes in lightning condition is among the most challenging problems.

\todo{deleted a chunk here, redo a transition and develop more on the topic}
Existing works for \gls*{lidar} mainly deal with simpler situations such as structured indoor or semi-structured city environments. We will therefore focus our attention on more challenging environments such as falling snow conditions or highly unstructured outdoor area. 

\todo{Explain how all sensors are affected by noise, and this will be the main topic of chapter 3 (in snowy condition)}

\todo{Write a paragraph for each chapter.}
The reminder of this document will be divided as follow: Chapter~\ref{chap:literature_review} will present a general literature review, Chapter~\ref{chap:lidar_snow} will be about the behavior of \gls*{lidar}s during snowfall and Chapter~\ref{chap:slam} will analyse the influence of different environments on place recognition performance before the conclusion.
