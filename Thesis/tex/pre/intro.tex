\chapter*{Introduction} \phantomsection\addcontentsline{toc}{chapter}{Introduction}

Modern robotics has experienced tremendous growth since its inception during the industrial revolution. Whether it is to assist humans in their works, to automate some tasks or to perform dangerous tasks, robots are emerging in a wide variety of applications. Recent technology such as the self-driving car project from Google and the BigDog quadruped robot from Boston Dynamics are feat of engineering that show the evolution of robotics from the not too distant past, when robots could only achieved simple tasks in controlled environments. One of the key elements that allowed for such progress, is the ability of the robot to adapt to changing environments. This is especially true for mobile robotics, where not only the surrounding environment can change, but the robot itself can move and must therefore be able to locate itself. This capability highly depend on algorithms that convert the raw inputs of available sensors into a convenient representation or abstraction of the environment; concept known as artificial perception.

A wide range of sensors are available on the market. Selecting these of these depends primarily on the specific problem to be solved and the available budget. 

\todo{Part to remove or make shorter}
A fairly inexpensive, yet powerful sensor for navigation is the \gls*{imu}. Composed of accelerometers, gyroscopes and sometimes magnetometers, it allows to infer position and orientation changes of the robotic platform, as well as gravitational forces. Although this type of sensor can be really precise over short distances, it accumulates error over time, which inevitably leads to a drift on the pose estimation. This problem can easily be solved by fusing \gls*{imu} data with those of another sensor for which the error is time independent. Probably due to its ease of use, the \gls*{gps} is among the most popular tool for this task. 

The major drawback of this sensor is that it requires to receive the signal of at least three satellites at all time. These signals can be blocked by building, terrain, dense foliage or other structures, thus causing important positioning errors or possibly no positioning at all. To work around this problem, you can use a map of landmarks representing the environment in which the robot moves. Whether this map is pre-existing or created progressively, the robot have to be equipped with sensors able to recognize these landmarks. Tactile sensor, infrared range finder, \gls*{sonar}, \gls*{radar} and multiple type of cameras are examples of such sensors.

These sensors capable of reacting to external stimuli are called exteroceptive sensors. In addition to their use for localization, the latter are needed to solve numerous other problems related to navigation: obstacles avoidance, traversability assessment and mapping just to name a few. Arguably, the most widely used of these sensors is the \gls*{rgb} camera. In addition to its price generally very low, cameras provides a rich data set on the scene in which the robot operates. Despite these obvious advantages, processing camera data generally rely on very complex algorithms that are often based on strong assumptions. The processing power and time required by those algorithms is not always available. The \gls*{lidar} is another sensor that provide rich data about the robot scene. It use laser to measure distances at different angles from the sensor center. Most \gls*{lidar}s provide \gls*{2d} set of points and can be mounted on a rotating unit to create \gls*{3d} data, while other can directly provide such data. \gls*{lidar}s are generally more expensive than cameras, but the geometrical information obtained therewith is often complementary to the appearance information provided by the cameras. For instance, a \gls*{lidar} will faithfully report the flat structure of a white wall, while the lack of visual features will make it impossible for the camera to infer such information. On the other hand, a camera would be able to locate itself using the rich appearance information of a poster on a wall, but the geometrical information of the wall retrieved by the \gls*{lidar} is of little help for the localisation task. Another important difference between those sensors is that the \gls*{lidar} is an active sensor that can be used day and night and which is mostly unaffected by lightning condition, while the camera is a passive sensor for which the compensation for changes in lightning condition is among the most challenging problems.

\gls*{rgb} cameras and \gls*{lidar}s are both widely use to solve multiple task related to mobile robotics navigation. Although there is still considerable research on the use of cameras, the literature for this sensor is significantly larger than for the more recent and expensive \gls*{lidar}s. This imbalance in the available literature is something that motivates us to better assess the potential of \gls*{lidar}s for robot navigation.  Furthermore, existing works for \gls*{lidar} mainly deal with simpler situations such as structured indoor or semi-structured city environments. We will therefore focus our attention on more challenging environments such as falling snow conditions or highly unstructured outdoor area. The reminder of this document will be divided as follow: Chapter~\ref{chap:literature_review} will present a general literature review, Chapter~\ref{chap:lidar_snow} will be about the behavior of \gls*{lidar}s during snowfall and Chapter~\ref{chap:slam} will analyse the influence of different environments on place recognition performance before the conclusion.
