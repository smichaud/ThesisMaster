\chapter*{Introduction} \phantomsection\addcontentsline{toc}{chapter}{Introduction}

Modern robotics has experienced tremendous growth since its inception during the industrial revolution. Whether it is to assist humans in their works, to automate some tasks or to perform dangerous tasks, robots are emerging in a wide variety of applications. Recent technology such as the self-driving car project from Google and the BigDog quadruped robot from Boston Dynamics are feat of engineering that show the evolution of robotics from the not too distant past, when robots could only achieved simple tasks in controlled environments.

One of the key elements that allowed for such progress, is the ability of the robot to adapt to changing environments. This is especially true for mobile robotics, where not only the surrounding environment can change, but the robot itself can move and must therefore be able to locate itself. This capability highly depend on algorithms that convert the raw inputs of available sensors into a convenient representation or abstraction of the environment; concept known as artificial perception.

A wide range of sensors are available to assist robots navigation. It is possible, for instance, to estimate the relative movements of the robot using wheel encoders. On the other hand, when the ground friction coefficient is variable or the load-gearing surface is very uneven, pose estimation relying on solely on this sensor is highly unreliable. Similarly, \gls*{imu} are composed of accelerometers and gyroscopes with which it is possible to infer changes in position. Although these sensors can be really precise over short distances, they accumulate error over time, which inevitably leads to a drift on the pose estimation.

The natural solution to this problem is to use sensors capable of providing absolute positioning. Arguably the most popular sensor providing such information is the \gls*{gps}. Then again, there are inherent problems with this sensor. \gls*{gps} requires to receive the signal of at least three satellites at all time. These signals can be blocked by building, terrain, dense foliage or other structures, thus causing important positioning errors or possibly no positioning at all.

Alternatively, a global map of landmarks can be use by the robot to localize itself. These landmarks are distinctive features of the environment acquired using exteroceptive sensors. Visual markers acquired with cameras, sounds signatures obtained with microphones or singular structures detected with sonars are examples of such features. In addition to solving localization and mapping problems, the use of such features make it possible to perform many other navigation tasks. For instance, it is possible to classify ground type to predict traversability or detect obstacles to compute path planning.

Arguably, the most widely used sensors for such task is the \gls*{rgb} camera. In addition to its price generally very low, cameras provides a rich data set on the scene in which the robot operates. Despite these obvious advantages, the images obtained by the cameras are produced by a projection and it is difficult or impossible to retreive information regarding the three dimensional structure of the scene. Furthermore, the quality of acquisitions depends greatly on the lighting conditions of the environment. The processing of these images for real-time navigation requires sufficiently powerful hardware, which is not always available on the robot.

The \gls*{lidar} is another sensor that provide rich data about the robot scene. It use laser to measure distances at different angles from the sensor center. Most \gls*{lidar}s provide \gls*{2d} set of points, but some are able to directly produce \gls*{3d} data, called point clouds. \gls*{lidar}s are generally more expensive than cameras, but the geometrical information obtained therewith is often complementary to the appearance information provided by the cameras. For instance, a \gls*{lidar} will faithfully report the flat structure of a white wall, while the lack of visual features will make it impossible for the camera to infer such information. On the other hand, a camera would be able to locate itself using the rich appearance information of a poster on a wall, but the geometrical information of the wall retrieved by the \gls*{lidar} is of little help for the localisation task. Another important difference between those sensors is that the \gls*{lidar} is an active sensor that can be used day and night and which is mostly unaffected by lightning condition, while the camera is a passive sensor for which the compensation for changes in lightning condition is among the most challenging problems. 

In Chapter~\ref{chap:literature_review} we will review existing literature about robot navigation. Because cameras were used in the field before LiDARs and because those sensors are somehow similar, techniques used with cameras inspired those for LiDARs. For this reason, there will be references related to cameras, but LiDARs proved to be an excellent choice for robot navigation and will be the sensors of interest of this document. As we will see, existing works for LiDAR mainly deal with simpler situations such as structured indoor or semi-structured city environments. We will therefore focus our attention on more challenging environments such as falling snow conditions or highly unstructured outdoor area. More precisely, we are interested in the impact of those complex environments on the resulting data and the task to be achieved.

Subsequently, in Chapter~\ref{chap:lidar_snow}, we will briefly present the basics of how \gls*{lidar}s operate and how readings are theoretically affected when scanning small structures. This type of structures is likely to be found in many complex environments where the robot might need to navigate. We are interested in quantifying the impact the latter on the sensor readings. For that matter, we placed four different \gls*{lidar}s so as to acquire data during falling snow conditions. Using our experimental setup, we are able to evaluate the influence of snowflakes of various sizes and falling at different rates on the sensors measurements. 

Before concluding, Chapter~\ref{chap:slam} provides a higher level analysis of the influence of the environment on a navigation algorithm. As for that, we chose a state-of-the-art \gls*{lidar}-based place recognition algorithm and we compared the results obtained in different environments. We created our own dataset in conditions similar to the one presented in the original article and acquired another dataset in forested area. Because forests are composed of multiple small structures such as branches and leaves, we considered the latter dataset more challenging and hypothesized that place recognition results will be negatively affected.
