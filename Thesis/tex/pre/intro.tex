\chapter*{Introduction} \phantomsection\addcontentsline{toc}{chapter}{Introduction}

Modern robotics has experienced tremendous growth since its inception during the industrial revolution. Whether it is to assist humans in their works, to automate some tasks or to perform dangerous tasks, robots are emerging in a wide variety of applications. Recent technology such as the self-driving car project from Google and the BigDog quadruped robot from Boston Dynamics are feat of engineering that show the evolution of robotics from the not too distant past, when robots could only achieved simple tasks in controlled environments.

One of the key elements that allowed for such progress, is the ability of the robot to adapt to changing environments. This is especially true for mobile robotics, where not only the surrounding environment can change, but the robot itself can move and must therefore be able to locate itself. This capability highly depend on algorithms that convert the raw inputs of available sensors into a convenient representation or abstraction of the environment; concept known as artificial perception.

A wide range of sensors are available to assist robots navigation. It is possible, for instance, to estimate the relative movements of the robot using wheel encoders. On the other hand, when the ground friction coefficient is variable or the load-gearing surface is very uneven, pose estimation relying on solely on this sensor is highly unreliable. Similarly, \gls*{imu} are composed of accelerometers and gyroscopes with which it is possible to infer changes in position. Although these sensors can be really precise over short distances, they accumulate error over time, which inevitably leads to a drift on the pose estimation.

The natural solution to this problem is to use sensors capable of providing absolute positioning. Arguably the most popular sensor providing such information is the \gls*{gps}. Then again, there are inherent problems with this sensor. \gls*{gps} requires to receive the signal of at least three satellites at all time. These signals can be blocked by building, terrain, dense foliage or other structures, thus causing important positioning errors or possibly no positioning at all.

Alternatively, a global map of landmarks can be use by the robot to localize itself. These landmarks are distinctive features of the environment acquired using exteroceptive sensors. Visual markers acquired with cameras, sounds signatures obtained with microphones or singular structures detected with sonars are examples of such features. In addition to solving localization and mapping problems, the use of such features make it possible to perform many navigation tasks. For instance, it is possible to classify ground type to predict traversability, detect obstacles to compute path planning or identify objects.

Interesting problem: 1. sensor depends on the condition, extracting such information efficiently is a complex task, rely on algorithm, good open problem, many possible upgrades, features must be distinctive enough and one most always be able to recognize it under different conditions. \todo{maybe put this after the next paragraph and incorporate the notion of noise changing data}

Arguably, the most widely used sensors is the \gls*{rgb} camera. In addition to its price generally very low, cameras provides a rich data set on the scene in which the robot operates. Despite these obvious advantages, processing camera data generally rely on very complex algorithms that are often based on strong assumptions. The processing power and time required by those algorithms is not always available. The \gls*{lidar} is another sensor that provide rich data about the robot scene. It use laser to measure distances at different angles from the sensor center. Most \gls*{lidar}s provide \gls*{2d} set of points and can be mounted on a rotating unit to create \gls*{3d} data, while other can directly provide such data. \gls*{lidar}s are generally more expensive than cameras, but the geometrical information obtained therewith is often complementary to the appearance information provided by the cameras. For instance, a \gls*{lidar} will faithfully report the flat structure of a white wall, while the lack of visual features will make it impossible for the camera to infer such information. On the other hand, a camera would be able to locate itself using the rich appearance information of a poster on a wall, but the geometrical information of the wall retrieved by the \gls*{lidar} is of little help for the localisation task. Another important difference between those sensors is that the \gls*{lidar} is an active sensor that can be used day and night and which is mostly unaffected by lightning condition, while the camera is a passive sensor for which the compensation for changes in lightning condition is among the most challenging problems.

\todo{Explain how all sensors are affected by noise, and this will be the main topic of chapter 3 (in snowy condition)}

\todo{deleted a chunk here, redo a transition and develop more on the topic}
Existing works for \gls*{lidar} mainly deal with simpler situations such as structured indoor or semi-structured city environments. We will therefore focus our attention on more challenging environments such as falling snow conditions or highly unstructured outdoor area. 

Before we proceed to the main topics of this document, a literature review will be presented in Chapter~\ref{chap:literature_review}. General articles related to the use of \gls*{lidar}s for navigation in complex environments as well as research papers more closely related to each of the following chapters will be discussed. 

Chapter~\ref{chap:lidar_snow} will be about the behavior of \gls*{lidar}s during snowfall. For this matter, we will analyse the effect of falling snow on the sensor readings. The idea is to determine how severely these weather condition can affect\dots 

Chapter~\ref{chap:slam} will analyse the influence of different environments on place recognition performance.

Thereafter the conclusion will present our \dots
