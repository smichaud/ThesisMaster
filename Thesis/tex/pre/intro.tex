\chapter*{Introduction} \phantomsection\addcontentsline{toc}{chapter}{Introduction}

Modern robotics has experienced tremendous growth since its inception during the industrial revolution. Whether it is to assist humans in their works, to automate some tasks or to perform dangerous jobs, robots are emerging in a wide variety of applications. Recent technology such as the self-driving car project from Google and the BigDog quadruped robot from Boston Dynamics are feat of engineering that show the evolution of robotics from the not too distant past, when robots could only achieved simple tasks in controlled environments. One of the key elements that allowed for such progress and that is always a hot topic in research, is the ability of the robot to adapt to changing environments. This is especially true for mobile robotics, where not only the surrounding environment can change, but the robot itself can move and must therefore be able to locate itself. This capability highly depend on algorithms that convert the raw inputs of available sensors into a convenient representation of the environment; concept known as robot perception.

A wide range of sensors are available on the market and the choice of these depends primarily on the specific problem to be solved and the available budget. A fairly inexpensive, yet powerful sensor for navigation is the \gls{imu}. Composed of accelerometers, gyroscopes and sometimes magnetometers, it allows to infer position and orientation changes of the robotic platform, as well as gravitational forces. Although this type of sensor can be really precise over short distances, it accumulates error over time, which inevitably leads to a drift on the pose estimation. This problem can easily be solved by fusing \gls{imu} data with those of another sensor for which the error is time independent. Probably due to its ease of use, the \gls{gps} is among the most popular tool for this task. The major drawback of this sensor is that it requires to receive the signal of at least three satellites at all time. These signals can be blocked by building, terrain, dense foliage or other structures, thus causing important positioning errors or possibly no positioning at all. To work around this problem, you can use a map of landmarks representing the environment in which the robot moves. Whether this map is pre-existing or created progressively, the robot have to be equipped with sensors able to recognize these landmarks. Tactile sensor, infrared range finder, \gls{sonar}, \gls{radar} and multiple type of cameras are examples of such sensors.

These sensors capable of reacting to external stimuli are called exteroceptive sensors. In addition to their use for localization, the latter are needed to solve numerous other problems related to navigation: obstacles avoidance, traversability assessment and mapping just to name a few. Arguably, the most widely used of these sensors is the \gls{rgb} camera. In addition to its price generally very low, cameras provides a rich data set on the scene in which the robot operates. Despite these obvious advantages, processing camera data generally rely on very complex algorithms that are often based on strong assumptions. The processing power and time required by those algorithms is not always available. The \gls{lidar} is another sensor that provide rich data about the robot scene. It use laser to measure distances at different angles from the sensor center. Most \gls{lidar}s provide \gls{2d} set of points and can be mounted on a rotating unit to create \gls{3d} data, while other can directly provide such data. \gls{lidar}s are generally more expensive than cameras, but the geometrical information obtained therewith is often complementary to the appearance information provided by the latter. For instance, a \gls{lidar} will faithfully report the structure of a white wall, while the lack of appearance will make it impossible for the camera to infer such information. However, a camera would be able to locate itself using appearance information of a poster on the wall, while the geometrical information of a flat wall could hardly help for the localisation task. Another important difference between those sensors is that the \gls{lidar} is an active sensor that can be used day and night and which is mostly unaffected by lightning condition, while it is very difficult for the passive camera to deal with changes in lightning condition \todo{rephrase that last sentence}.

\gls{rgb} cameras and \gls{lidar}s are both really well suited for multiple task related to mobile robotics navigation. Although there is still considerable research on the use of cameras, the literature for this sensor is significantly larger than for the more recent and expensive \gls{lidar}s. This is one reason that motivates us to better assess the potential of \gls{lidar}s for this task. Existing works for that sensor mainly deal with simpler situations such as structured indoor or semi-structured city environments. We will therefore focus our attention on more challenging environments such as falling snow conditions or highly unstructured outdoor area. \todo{rephrase and add references} Chapter 1 is about \gls{lidar} Characterization, chapter 2 is about traversability and the third chapter is about place recognition.
