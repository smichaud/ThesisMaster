\chapter*{Conclusion}
\phantomsection\addcontentsline{toc}{chapter}{Conclusion}

The objective of this document was to analyse the influence of complex environments on different \gls*{lidar}s used in a context of mobile robotics navigation. Because \gls*{lidar}s can effectively retrieve the three-dimensional structure of an environment, they can advantageously be used for several navigation tasks, such as obstacles avoidance, localization and mapping. The information they provide is complementary to that of other sensors such as cameras, which capture information on the appearance of the surrounding scene. Although the literature on mobile robotic navigation is rich, most existing research deal with indoor or structured outdoor environment. Dealing with complex environments, such as forest or falling snow conditions, remains a important problem. Therefore, analyzing how such complex environments influence the \gls*{lidar}-based navigation can help to develop algorithms that are more robust to changing conditions.

In Chapter~\ref{chap:lidar_snow}, we focused our attention on the impact of falling snow on \gls*{lidar} data. For that matter, we first presented an overview of \gls*{lidar}s functioning, which helped understand what could potentially cause noise in the data. The first issue raised was that, when the spot produced by the laser beam is not completely on a single target, a method of inference of the distance must be chosen and may not be suitable for the application. This is generally caused by object edges of small particles. The second issue raised is \gls*{lidar}s acquire data points sequentially, which can lead to a distorted representation of the scene when the environment is dynamic. Consequently, the dynamic nature and small size of snowflakes make snowfall a perfect example of challenging condition for \gls*{lidar} acquisition.

To perform our analysis of the impact of falling snow on \gls*{lidar} acquisition, we collected data from four sensors simultaneously, namely the Velodyne HDL-32E, the SICK LMS151, the SICK LMS200 and the Hokuyo UTM-30LX-EW. We acquired data during six snowfalls in a wide variety of condition. The final dataset used for our analysis contains more than 40 hours of data. 

For our analysis, we first observed how the proportion of return caused by snowflakes evolved over time. We visually represented the short-term evolution by overlaying the snowflakes according to their spatial arrangement for four subsequent scans. This shown significant quantitative and spatial changes, even over such short time. We observed no pattern in the distribution of snowflakes and believe it can be modeled by a random process. We also illustrated the long-term evolution by showing the proportion of return as function of time, for the whole duration an acquisition. The shape of this curve provides information on the progress of the weather during this period. Additionally, we conducted a comparative analysis of the overall sensitivity to snowflakes of our four sensors. We showed that the SICK LMS200 is the more sensitive with peaks reaching up to up to \SI{15}{\percent} of echoes caused by falling snow, while others three \gls*{lidar}s never exceeded \SI{1}{\percent}. Beside our temporal analysis, we analysed how range can affect the probability to trigger a measurements. Based on a histogram, we found that the probability density function is close to a log-normal. Our final important finding is that upon \SI{10}{\meter}, snowflakes no longer seem to trigger measurement.

In Chapter~\ref{chap:slam}, we have conducted our analysis at a higher level, by comparing the performance of a state-of-the-art \gls*{lidar}-based place recognition algorithm in different environments. There are two main reasons for this algorithm choice. Firstly, the ability of a robot to identify previously visited places allows to solve several other navigation problems, such as multi-session-mapping, kidnapped robot and \gls*{slam}. Secondly, the chosen algorithm proved to be successful in indoor and structured outdoor environments, but have not been tested in unstructured environments.

For our comparative analysis, we acquired datasets in two different area of the Laval University campus using the Husky A200 mobile robot. The first area is our model for a structured outdoor environment, similar to those on which the original algorithm was tested. In this location, the ground is mostly flat and the scanned space contains several man-made objects, such as buildings, stairs and lampposts. The second area, representing our unstructured environment, is in a forest where the ground is uneven in some parts. We also used two different sensors for our acquisitions, namely the SICK LMS151 and the Velodyne HDL-32E. We used the resulting Structured-SICK, Unstructured-SICK and Unstructured-Velodyne datasets to evaluate the impact of both the environment and the sensor choice on the place recognition performance.

Before performing the comparative analysis itself, we presented some fundamental concepts required to understand the chosen algorithm. This include features keypoints and descriptors, range images and scans comparison techniques. We then proceed to explain the algorithm itself, which first convert point clouds into range images in order to extract NARF features and then use a mixture of \gls*{bow} and features matching to find possible transformations between two scans. The final output of the algorithm is a score computed from the best evaluated transformation betweens two scans, and represent the confidence of the system that those scans originate from the same place. 

For our results presentation, we showed the relation between the algorithm output score and the distance separating the input scans. We consider that the closer two scans are from each other, the more likely they are to represent the same place. Therefore, we expected an inverse relation between the score and the distance separating the scans. We also presented the results using binary labels, because practical uses of place recognition generally required pairs of scans to be identified as originating from the same place or not. For that matter, all scans for which the distance between them was below a threshold were considered as belonging to the same place. Similarly, we considered that if the score obtained for a pair of scans was above a threshold, the algorithm labeled the input scans as representing the same place. This labeling also enabled us to identify \gls*{fp} and calculate the recall for different thresholds combinations. As we explained, \gls*{fp} must be avoided during \gls*{slam}, because they create false loop closures, which distort the resulting map. 

The presentation of results, as described above, enabled us to make our final observations. As expected, we showed a relation of the form $f(x)=1/x$ between the algorithm output score and the distance between two scans. This was true for our three datasets. However, some outliers were observed for the unstructured environment, whereas this was not the case for the structured environment. This is important when using the discretized labels, because such outliers can result in \gls*{fp} or, if the score threshold is adjusted to avoid them, reduce significantly the recall. Our last observation is that the score decreases more rapidly as a function of the distance between scans in the unstructured environment than in the structured environment. This means that the algorithm become more quickly sensitive to disturbances and noise as the place recognition range increases in an unstructured environment than in a structured environment. The same observation applies when comparing the sensors, in which case the algorithm become sensitive faster as the range increases for the Velodyne than the SICK.

To conclude, the goal of this document was to evaluate the influence of complex environments on \gls*{lidar}-based robot navigation. By characterizing \gls*{lidar}s data acquired during snowfalls and by comparing performances of \gls*{lidar}-based place recognition in forest and in structured outdoor environment, we showed that these environments can impact negatively robot navigation. Based on our observations, future research could focus on finding methods to increase robustness to these conditions, for instance by filtering input data. In the case of our place recognition analysis, a better identification and quantification of the sources causing performance decrease could also help the development of better solutions for navigation.
