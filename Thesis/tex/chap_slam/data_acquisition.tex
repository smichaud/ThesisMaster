\section{Data Acquisition}
\label{sec:chap_slam_data_acquisition}

In the following section, we first describe the robotic platform and sensors used to gather our place recognition dataset. We then describe in more details the dataset acquisition procedure and the resulting data.


\subsection{Robotic Platform}
\label{ssec:chap_slam_platform}

The Husky A200 is a medium size (\SI{990 x 670 x 390}{\milli\meter}) \gls*{ugv} developed by \cite{ClearpathWeb}. It is a rugged robot designed for all terrain conditions, therefore well suited for our experiments in the forest. It uses a differential-drive skid steer allowing easy control and in place turns for our data acquisition. It has a maximum speed of \SI{1}{\meter\per\second}, but we generally acquire data at a lower speed to minimize odometry and range data errors. This platform, with a maximum payload capability of \SI{75}{\kilo\gram}, can easily carry all sensors used for our experiments. Our platform is shown in Figure~\ref{fig:chap_slam_husky}. More details about the available devices are presented in Table~\ref{tab:husky_devices}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\linewidth]{img/chap_slam/husky_labelled.png}
    \caption[The robotic platform (Husky A200) and the devices used for data acquisition.]{The robotic platform (Husky A200) and the devices used for data acquisition. The SICK \gls*{lidar} is not mounted, but is shown on the bottom right corner of the figure. Note that the \gls*{ptu} is hidden by a cover and the on-board computer is mostly occluded by the Velodyne \gls*{lidar}.}
    \label{fig:chap_slam_husky}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Device} & \textbf{Manufacturer}       & \textbf{Model}  & \textbf{Use}                          \\ \hline
        Computer        & --                          & --              & Data acquisition/synchronisation      \\
        Gateway         & Microhard System Inc.       & VIP2400         & Network and Wi-Fi communication       \\
        Gamepad         & Logitech                    & F710            & Remote control of the robot movements \\
        \gls*{lidar}    & Velodyne                    & HDL-32E         & Point cloud acquisition               \\
        \gls*{lidar}    & SICK                        & LMS151          & Point cloud acquisition               \\
        \gls*{ptu}      & FLIR Motion Control Systems & D46-17          & Rotate the SICK \gls*{lidar}          \\
        \gls*{imu}      & ChRobotics                  & UM6             & Odometry                              \\
        Camera          & Axis                        & M1013           & Visual reference                      \\
        \gls*{gps}      & NovAtel                     & SMART6          & Not used                              \\
        \bottomrule
    \end{tabular}
    \caption[List of devices available on the Husky A200 and their use in our experiments.]{List of devices available on the Husky A200 and their use in our experiments. Note that both \gls*{lidar}s were never mounted at the same time.}
    \label{tab:husky_devices}
\end{table}

The on-board computer (2.4 GHz Intel i5-520M) is an essential element of our experiments, as it connects all devices, acts as a control interface for the robot and stores the acquired data. The computer does not provide a \gls*{gui}, but it is connected to the gateway that broadcasts a WiFi network, allowing \gls*{ssh} communication. The platform is also equipped with a wireless gamepad, which enables manual control of the movements of the robot. Point clouds acquisition is possible using either the SICK LMS151 or the Velodyne HDL-32E. The selected sensor is mounted on the \gls*{ptu}, which remains fixed for the Velodyne, but is rotated with the SICK to merge multiple \gls*{2d} scans into a single \gls*{3d} point cloud. Section~\ref{ssec:chap_slam_platform} gives more details about the resulting point clouds for each sensor.

The sensors described above are essential for our place recognition research, but we also use the wheel encoder along with the \gls*{imu} for odometry estimation and the camera for visual reference of the dataset. Note that we do not use the \gls*{gps}, as we do not want to depend on it for the task of place recognition.


\subsection{Dataset Description}
\label{ssec:chap_slam_platform}

Data acquisition was performed using \gls*{ros}, a set of software libraries and tools created to simplify the development of robotics applications. It provides all drivers for the Husky and our sensors out of the box. Its data publishing system provides timestamps that allows easy synchronization between sensors. The recording tool (rosbag) was used to create our datasets, with data processing performed \textit{a posteriori}.

We produced datasets in two different areas of the Laval University campus. An aerial overview of the path followed by the robot at these two locations is presented in Figure~\ref{fig:chap_slam_path}.

The first site was chosen for its more structured nature and is located between the Alexandre-Vachon and the Adrien-Pouliot buildings. This environment is mostly open, the terrain is smooth and flat and the site contains man-made objects such as buildings, stairs or tables. Examples of pictures acquired by the robot on this site are presented in Figure~\ref{fig:slam_view_building1} and~\ref{fig:slam_view_building2}. This dataset closely resembles the kind of data on which several place recognition algorithms are typically tested on (e.g Freiburg Campus 360 degree 3D scans~\citep{FreiburgDataset}, Robotic 3D Scan Repository~\citep{Datasets}). We will use this data to validate our method.

The second site, chosen for its unstructured nature, is located in a wooded area, also on the Laval University campus. The dataset path starts on a pedestrian walkway, but after its second turn (of approximately \SI{330}{\degree}), it continues for around \SI{100}{\meter} in rougher terrain. This forested environment presents multiple small structures and a lot of occlusions. Figure~\ref{fig:slam_view_forest1} and Figure~\ref{fig:slam_view_forest2} show pictures from the robot camera at this location. This dataset will allow us to better understand the influence of a less structured environment on the place recognition algorithm. In particular, the absence of large flat surfaces and corners typical of buildings, as well as the closeness of the space, will be challenging for place recognition algorithms.

\begin{figure}[H]
    \centering
    \subfloat[]{\label{fig:path_overview}}{\includegraphics[width=0.995\linewidth]{img/chap_slam/paths.jpg}}\\ \vspace{3mm}
    \subfloat[]{\label{fig:path_building}}{\includegraphics[width=0.485\linewidth]{img/chap_slam/path_building_arrow.png}} \hspace{2mm}
    \subfloat[]{\label{fig:path_forest}}{\includegraphics[width=0.485\linewidth]{img/chap_slam/path_forest_arrow.png}}
    \caption[Partial aerial view of the Laval University campus including the two approximate paths followed by the robot for data acquisition.]{\protect\subref{fig:path_overview} Partial aerial view of the Laval University campus including the two approximate paths followed by the robot for data acquisition. \protect\subref{fig:path_building} Zoomed view of the path followed (counterclockwise from tag 1) in a structured environment. The length of this path is approximately \SI{160}{\meter}. \protect\subref{fig:path_forest} Zoomed view of the path followed (clockwise from tag 2) to create the forest dataset. The length of this path is approximately \SI{275}{\meter}. Images source: Google Earth, (2015)}
    \label{fig:chap_slam_path}
\end{figure}

\begin{figure}[H]
    \centering
    \subfloat[]{\label{fig:slam_view_building1}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/building_view1.jpg}} \hspace{2mm}
    \subfloat[]{\label{fig:slam_view_building2}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/building_view2.jpg}}
    \subfloat[]{\label{fig:slam_view_forest1}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/forest_view1.jpg}} \hspace{2mm}
    \subfloat[]{\label{fig:slam_view_forest2}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/forest_view2.jpg}}
    \caption[Images from the \gls*{ugv} camera during the acquisition of the two datasets.]{Images from the \gls*{ugv} camera during the acquisition of the structured dataset \protect\subref{fig:slam_view_building1} \protect\subref{fig:slam_view_building2} and the unstructured dataset \protect\subref{fig:slam_view_forest1} \protect\subref{fig:slam_view_forest2}.}
    \label{fig:slam_views}
\end{figure}

Besides the influence of the type of environment, we are also interested in the impact of the sensor used and data associated with it. For this analysis, we used two sensors, namely the SICK and the Velodyne, for which you can find details in Table~\ref{tab:slam_sensor_resolution}.

The SICK is a \gls*{2d} \gls*{lidar} with a scanning angle of \SI{270}{\degree}, a resolution of \SI{0.5}{\degree} and an acquisition frequency of \SI{50}{\hertz}. This sensor was placed on the \gls*{ptu} so that the dead angle faced downward. During acquisition, it was rotated around the vertical axis (pan) at a speed of \SI{14.32}{\degree\per\second} for half a turn, while the vehicle was stationary. This procedure allowed the acquisition of 628 \gls*{2d} scans of \SI{540}{\points}, later merged into a single \gls*{3d} point cloud.

The Velodyne, for its part, directly allows \gls*{3d} point cloud acquisition by spinning 32 lasers around its vertical axis. These lasers are evenly distributed between \SI{-30.67}{\degree} and \SI{10.67}{\degree} relative to the horizontal plane. According to the Velodyne datasheet \citep{VelodyneDatasheet}, the device acquires approximately \SI{700000}{\points\per\second} and publishes at \SI{10}{\hertz}, therefore creating point clouds of \SI{70000}{\points}. Note that these point counts are variable as the rotation speed can change slightly and the use of the \gls*{udp} can lead to some loss of points. 

To create the dataset using the SICK, the robot was controlled to follow the established path and it was stopped at regular interval to acquire scans. Regarding the acquisition using the Velodyne, the robot was driven at a constant speed (\SI{0.3}{\meter\per\second}) and perform data acquisition simultaneously. The quantity of scans thus produced being too high, the data set has been sub-sampled thereafter.

Table~\ref{tab:slam_sensor_resolution} summarizes information about the sensors resolution and \gls*{fov} while Table~\ref{tab:slam_datasets} provide general information about each of our datasets.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}lrrrrr@{}}
        \toprule
    \makecell[lc]{\textbf{Sensor}}& \makecell[cc]{\textbf{Horizontal}\\\textbf{resolution}} & \makecell[cc]{\textbf{Vertical}\\\textbf{resolution}} & \makecell[cc]{\textbf{Minimum}\\\textbf{angle}} & \makecell[cc]{\textbf{Maximum}\\\textbf{angle}} & \makecell[cc]{\textbf{Point}\\\textbf{counts}} \\
        \hline
        SICK LMS151      & \SI{0.57}{\degree} & \SI{0.5}{\degree}  & \SI{-45.00}{\degree}  & \SI{90.00}{\degree}  & 339120 \\
        Velodyne HDL-32E & \SI{0.16}{\degree} & \SI{1.33}{\degree} & \SI{-30.68}{\degree}  & \SI{10.67}{\degree}  & 72000  \\
        \bottomrule
    \end{tabular}
    \caption[Details about the point clouds created with our two \gls*{lidar}s.]{Details about the point clouds created with our two \gls*{lidar}s. The minimum and maximum angles are given relative to an horizontal plane in the sensor frame of reference and both sensors report \SI{360}{\degree} around the vertical axis. The point counts represent the maximum number of points in the resulting point cloud.}
    \label{tab:slam_sensor_resolution}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{@{}lllrr@{}}
        \toprule
        \textbf{Site}  & \textbf{Sensor}   & \textbf{Date} & \makecell[cc]{\textbf{Number of scans}\\\textbf{for loop 1}} & \makecell[cc]{\textbf{Number of scans}\\\textbf{for loop 2}} \\
        \hline
        Structured     & SICK LMS151       & July 16\textsuperscript{th}, 2015 & 81  & 83  \\
        Unstructured   & SICK LMS151       & July 14\textsuperscript{th}, 2015 & 94  & 92  \\
        Unstructured   & Velodyne HDL-32E  & May 28\textsuperscript{th}, 2015  & 277 & 276\\
        \bottomrule
    \end{tabular}
    \caption[Datasets acquired for place recognition analysis.]{Datasets acquired for place recognition analysis.}
    \label{tab:slam_datasets}
\end{table}

Examples of point clouds from our datasets are shown in Figure~\ref{fig:slam_pointclouds}. We observe that the unstructured environment is more congested and objects are closer to the sensor than for the structured environment. In fact, the average distance of points from the sensor in the unstructured environment is \SI{7.61}{\meter} compared to \SI{9.23}{\meter} for the structured environment. There is also more space without obstacle in the sensor range for the structured dataset. The percentage of points return using the SICK is \SI{82.6}{\percent} on average in the forest dataset compared to \SI{40.2}{\percent} in the structured dataset. These point clouds also illustrate the higher vertical resolution and larger \gls*{fov} of the SICK compared to the Velodyne. The influence of these factors will be discussed in Section~\ref{sec:chap_slam_results}.

\begin{figure}[H]
    \centering
    \subfloat[]{\label{fig:slam_pointcloud_building1}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/pointcloud_building1.png}} \hspace{2mm}
    \subfloat[]{\label{fig:slam_pointcloud_building2}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/pointcloud_building2.png}}
    \subfloat[]{\label{fig:slam_pointcloud_forest1}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/pointcloud_forest1.png}} \hspace{2mm}
    \subfloat[]{\label{fig:slam_pointcloud_forest2}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/pointcloud_forest2.png}}
    \subfloat[]{\label{fig:slam_pointcloud_velodyne1}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/pointcloud_velodyne1.png}} \hspace{2mm}
    \subfloat[]{\label{fig:slam_pointcloud_velodyne2}}{\includegraphics[width=0.47\linewidth]{img/chap_slam/pointcloud_velodyne2.png}}
    \caption[Examples of point clouds from our datasets viewed from different perspectives.]{Examples of point clouds from our datasets viewed from different perspectives. \protect\subref{fig:slam_pointcloud_building1} and \protect\subref{fig:slam_pointcloud_building2} show acquisition from the SICK in the structured environment. \protect\subref{fig:slam_pointcloud_forest1} and \protect\subref{fig:slam_pointcloud_forest2} also show point clouds from the SICK, but in the unstructured environment. Finally, \protect\subref{fig:slam_pointcloud_velodyne1} and \protect\subref{fig:slam_pointcloud_velodyne2} show the result of acquisition from the Velodyne in the unstructured environment. Robot position is represented by a red sphere with a \SI{1}{\meter} radius in each picture.}
    \label{fig:slam_pointclouds}
\end{figure}

