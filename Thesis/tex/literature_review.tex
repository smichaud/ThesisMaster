\chapter{\chapzerotitle}
\label{chap:literature_review}


\section{Introduction}
%The literature review is an essential step of the research process. In addition to increasing the wealth of knowledge in the field, it helps to ensure the relevance of the targeted contributions relative to existing work. 
In this thesis, we are interested in the influence of complex situations, either complex unstructured environments or challenging weather conditions, on \gls*{lidar}-based robot navigation. This is a high-level problem involving several subtopics. Therefore, we will start our literature review with a general overview of the work on \gls*{lidar} functioning (Section~\ref{sec:literature_lidar_functionning}) and navigation in complex environments (Section~\ref{sec:literature_complex_environments}).  Subsequently, in Section~\ref{sec:literature_snow}, we will discuss research related to navigation in snowfall conditions, which is the linked to Chapter~\ref{chap:lidar_snow}. Finally, to contextualize Chapter~\ref{chap:slam}, Section~\ref{sec:literature_slam} will present papers related to feature-based place recognition and navigation in forest environments. 


\section{Context 1}
\label{sec:literature_lidar_functionning}

\gls*{lidar}s use laser to get distance, generally acquire (a collection of) multiple data point to create 2d/3d structure representation.
A general understanding of the physic behind the lidar helps understand what could possibly go wrong and how to prevent it.
More details and figures (brief overview for what interest us) in Chapter~\ref{chap:lidar_snow}, specifically Section~\ref{sec:chap_lidar_basics}.
Also, because there is multiple lidar available, choose depends on requirements (2d/3d, range, weight).


~\cite{Bosch2001}:
Review usual lidar techniques (pulsed, phase-shift and frequency modulated continuous wave), basics then limitations.
~\cite{Pomerleau2015a}:
Figure with different 3d sensors with weight and max range. small for on-board indoor, big large range for airborne


\section{Context 2}
\label{sec:literature_complex_environments}

Start with a navigation definition... Which include stuff like ... Obstacle avoidance/traversability for path planning, localisation and mapping... 
Give some insight for what might be a "complex" environment (there is a gradation from completely controlled, to indoor, to outdoor, to forest etc.)

\subsubsection{Path Planning}
Geolocation, obstacles avoidance / traversability (including ground classification) = path planning

~\cite{Hussein2015}:
Geolocation of ugv in forest (horizontal position) by finding geometric match between a map of observed tree stems using onboard lidar with a map generated from the structure of thee crowns from high resolution aerial orthoimagery of the forest canopy. No need for a priori knowledge of the area surrounding the robot, only input is the geometry of detected tree stems.

~\cite{Wurm2009}:
This paper present a technique that use laser remission values to detect grass like vegetation. They use a self-supervised classification vibration based. (include in the path planning section)

\subsubsection{Localisation and Mapping + SLAM}
Start with 2d solutions. (Gmapping), but can't deal with 3D conditions or uneven bearing surface. Hector propose a 2d + imu that slighly handle uneven floor.
State that 2D solutions for slam pretty much meet our needs, they are able to yield precise results in real time\dots

~\cite{Grisetti2007}: Gmapping,
They use a particle filter to solve SLAM (grid map). Each particle carry an individual map. They take into account the movement but also the most recent observations, which decrease uncertainty. They use selective selective resampling to reduce the problem of particle depletion. They do indoor AND outdoor. 

~\cite{Kohlbrecher2011}: Hector mapping,
Focus on search and rescue. Learn map of unknown environments, occupancy grid, low computational resources. Lidar + IMU. Fast approximation of map gradients and multi resolution grids. Pretend they don't need explicit loop closure\dots


Then they start using 3D, this is more applicable to complex environments (remove some assumptions such as even ground).
Quickly explain ICP, can be used for small steps localisation, but converge in minimum and can't be used directly for localisation (for any given input scan)

~\cite{Besl1992}:
Original ICP article

~\cite{Pomerleau2015a}:
A Review of Point Cloud Registration Algorithms for Mobile Robotics\dots

~\cite{Colas2013}:
Article doing path planning in complex environments for search and rescue based on ICP for 3D terrains. 

~\cite{Pomerleau2013}:
This article present an analysis of ICP variants for real world applications. They also propose an open-source icp library that operate in real time.  

The Marulan Data Sets~\citep{Peynot2010}: state that, even if not snow, similar because of particles and could be used for assessing algorithm or do similar statistical studies for comparison. 


\section{Snowfall Conditions}
\label{sec:literature_snow}

Snowfall conditions are challenging because snowflakes cause occlusion and, because of their small size and dynamic nature, they tend to produce noise in sensors acquisitions. While it is often possible to avoid navigating in these conditions, some robots will inevitably face this situation in order to perform the tasks for which they were designed. For instance, ~\citet{Moorehead_1999_2122} aim at develop a robot to search and classify meteorites in Antartica. In the experiments, the \gls*{ugv} drives autonomously for \SI{10.3}{\kilo\meter} in different weather and terrain conditions. The navigation is based on stereo camera images and single line \gls*{lidar} scans. In this article, the authors explain that the snow-covered surfaces do not provide a lot of visual cues, which makes stereo cameras unreliable. For this reason, the robot take advantage of the \gls*{lidar}, mainly for obstacle avoidance, but often as single sensor. In return, the authors stated that part of the experiments \enquote{was performed during heavy snow which made the laser useless}, and therefore used alternative solutions. 

Another example of robots that require to handle all-weather conditions are those deployed on the battlefield. \citet{yamauchi2010fusing} uses ultra-wideband radar, stereo camera and \gls*{lidar} data for navigation. According to the authors, \enquote{[\gls*{lidar}] and stereo vision provide greater accuracy and resolution in clear weather but has difficulty with precipitation and obscurants}. The ultra-wideband radar has the ability to see through small particles such as snow, rain and fog and is therefore complementary to others sensors used. The final system achieve good results by using traditional sensor fusion, as well as a selective use of the sensors which are activated or deactivated depending on the conditions. 

\citet{sumi-arso-13}, for their part, aimed to evaluate sensors for personal care robots in natural lighting and falling snow conditions. For that matter, they built two simulators that reproduce those conditions and used three type of sensor: an active stereo sensor (Microsoft Kinect), a \gls*{lidar} (Mesa SR4000) and a passive stereo sensor (PGR Bumblebee2). This approach is interesting because it allows to control various parameters that may affect the sensor readings. By cons, information related to our research, that is to say, the impact of snow flakes on \gls*{lidar} measurements, was quite limited. In addition, the physical properties of simulated snowflakes can be different from that of real snowflakes, leading to inaccurate results for real applications.

Article by ~\citet{barnum2010analysis} is probably the most similar to the work that we will present in Chapter~\ref{chap:lidar_snow}, but applied to video rather than \gls*{lidar}s data. They explain that rain and snow are dynamic process, which cause spatial and temporal fluctuations in videos. Although the spatio-temporal changes appear to be chaotic, they predict the overall effect of these conditions on the video, in frequency space. This modeling of the effect of weather conditions on videos improve noise filtering for features extraction when compared to previous pixel-based or patch-based methods.

Finally, \citet{servomaa2002snowfall} have installed a set a of sensors for snowfall observation. The system is composed of a radar and a \gls*{lidar} that record the atmospheric profile up to \SI{6000}{\meter} and another radar along with two balances to record snowfall at ground level. Although the installation are fixed and the \gls*{lidar} is used only to evaluate transition in cloud conditions, they propose techniques for to estimate snowfall characteristics. These techniques may advantageously be used to adapt the behavior of the robot depending on weather conditions.  

%~\cite{lever2013autonomous}: ice sheet Antartica and Greenland Earth's climate history, life in extreme environments and the evolution of the cosmos, remote = expensive, ground-penetrating radar to detect crevasse of void. WOFFFF....


\section{Feature-Based Place Recognition and Forest Environments}
\label{sec:literature_slam}

As we will see in more details in Chapter~\ref{chap:slam}, place recognition is a useful tool for mobile robot navigation. A robot can determine whether he is in previously visited place or not by comparing its current sensor acquisition with those acquired earlier. Raw sensor data is usually converted into a representation that better capture the key information before it is used for comparison. This process is known as features extraction and consist in identifying points of interest from the sensor data (e.g. image), called feature keypoints, and create a descriptor for each keypoint. A descriptor is a vector containing values which should represent the area surrounding the keypoint robustly, even under small disturbance such as different lighting conditions or change of viewpoint. The popularity of using features representation can be attributed to the field of computer vision, especially with the introduction of SIFT~\cite{Lowe2004} and SURF~\cite{Bay2006}. This concept has since been applied to 3D data. Table~\ref{tab:features_examples} present some popular \gls*{2d} and \gls*{3d} features. 

Although the choice of features can influence the performance of place recognition, we will not address this issue directly in this thesis. Instead, we analyse the impact of the environments on the overall place recognition performance. For those interested in this topic, there are articles that present comparative evaluation of different features. \citet{Filipe2014} present an evaluation of \gls*{3d} keypoint detectors, more precisely for RGB-D objects. In this paper, they focus on the invariance of keypoints detectors to rotations, scales and translation. Similarly, \citet{Boyer2011} propose a benchmark that allows to evaluate how different algorithms for retrieving keypoints and descriptors react to different transformations.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Type of data}  & \textbf{Keypoint/descriptor} & \textbf{Name}               & \textbf{Reference} \\
        \hline
        Image                  & Keypoint                     & Harris and Stephens corners & \cite{Harris1988}  \\
        Image                  & Both                         & SIFT                        & \cite{Lowe2004}    \\
        Image                  & Both                         & SURF                        & \cite{Bay2006}     \\
        \gls*{3d}              & Descriptor                   & FPFH                        & \cite{Rusu2009}    \\
        \gls*{3d}              & Both                         & ISS                         & \cite{Yu2009}      \\
        \gls*{3d}              & Descriptor                   & SHOT                        & \cite{Tombari2010} \\
        \gls*{3d}              & Both                         & NARF                        & \cite{Steder2011a} \\
        \bottomrule
    \end{tabular}
    \caption[Examples of popular descriptors and keypoints detectors for images and \gls*{3d} data.]{Examples of popular descriptors and keypoints detectors for images and \gls*{3d} data. Some \gls*{2d} keypoints have been adapted for \gls*{3d} such as Harris and Stephens and SIFT.}
    \label{tab:features_examples}
\end{table}

The use of such characteristics is used for many years to solve the problem of place recognition, but first makes its appearance with approaches using cameras~\citep{Torralba2003}, \cite{Ulrich2000}. The most noteworthy example is probably the work of~\citet{Cummins2008}, commonly referred as FAB-MAP. They use a probabilitic framework to recognize previously seen places and identify new places. The algorithm is able to recognize the redundant visual information that does not significantly help to distinguish places, which is use to reduce the probability of such examples to be labeled as originating from the same place. They reach recall of \SI{48}{\percent} at \SI{100}{\percent} precision for a dataset of \SI{1.9}{\kilo\meter}. The authors also proposed an enhanced version of the algorithm~\cite{Cummins2011} in which they focus on scalability primarily using the concept of inverted index. They are able to reach \SI{48}{\percent} recall at \SI{100}{\percent} precision on a \SI{70}{\kilo\meter} dataset.

By cons, existing place recognition algorithms based only on \gls*{3d} \gls*{lidar} data are quite limited. In our knowledge, \citet{Magnusson2009} are the first to address this problem. They use normal distribution transform (NDT) to create feature histograms based on surface orientation and smoothness to represent each scan. They also align scans with respect to dominant surface orientation to achieve rotation invariance. Finally they use expectation maximization to automatically determine the treshold that separate corresponding from non-corresponding scans. They achieve recall rates between \SI{22.9}{\percent} and \SI{69.6}{\percent} with false-positive rates below \SI{1.17}{\percent} for three different datasets. More recently, ~\citet{Mack2015} proposed a similar approach for solving the place recognition problem based on \gls*{3d} lidar data. The authors indicated that they use simpler histograms and a different distance metric to achieve similar results than~\citet{Magnusson2009}. The main contributions of the algorithm are that is it simpler to implement and less computationally demanding.

The two previously mentioned algorithms use global descriptors (i.e. a single descriptor for each scan), which is often faster to process but less robust to local disturbances than local features (i.e. a set of feature keypoints and associate descriptors for each scan). For our experiements in Chapter~\ref{chap:slam}, we will use the algorithm presented by~\cite{Steder2011b}, which is an extension of their previous work~\cite{Steder2010}. The algorithm, that will be presented in more details in Section~\ref{sec:chap_slam_algo}, use a mixture of \gls*{bow} and features matching to recognize places.

%Note that global descriptors are also used in computer vision, for instance the GIST global descriptor proposed by~\citet{oliva2001modeling}, but are more oriented toward general categorization or fast processing.

In our knowledge, none of the previously discussed algorithms have been tested in forest environments or have been developed for specific conditions. When the environment in which the robot will operate is known in advance, algorithms can be developed using this prior knowledge to potentially improve performance. For instance, one could intuitively assume that in forest, tree trunks are more reliable features than foliage. ~\citet{Latulippe2013} proposed to use machine learning to filter local point cloud features in natural environments for scans alignment purpose. In their paper, they indeed concluded that features produced in foliage region are not reliable. Other examples of prior knowledge used for \gls*{lidar}-based navigation in forest include ~\citep{Lalonde2006}, which present a technique for segmenting data in three classes and~\cite{Mcdaniel2012}, which present a technique for segmenting ground and trees. These segmented regions of data can be use to identify navigable area or be used as features for multiple tasks.

~\cite{Song2012}:
They proposes a localisation solution in forested environments. They use the largest group of approximately parallel tree trunk features to align successive scans along five dimensions (except z). Optimal transformation determined based on the axes of two tree pairs. Then they align ground by minimizing the differences of points shared in the cells. (they basically do ICP, but they pretend to remove problems such as initial transformation and noisy scene.) (This is high level engineered features, could use this for smooth transition in the text)

~\cite{Iagnemma2012}:
The method used in the previous article to extract tree truck parameters from point cloud. 3 purely geometric steps: First, the raw point clouds are segmented by utilizing the circular shape of trees, and segments are grouped into tree sections based on the principle of spatial proximity. Second, circles and axes are extracted from tree sections which are subject to loss of shape information. Third, by clustering and integrating the tree sections resulted from various space inconsistencies, straight tree trunk landmarks are finally formed for future localization. The experimental results from real forested environments are presented.

