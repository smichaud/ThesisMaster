\chapter{\chapzerotitle}
\label{chap:literature_review}


\section{Introduction}
%The literature review is an essential step of the research process. In addition to increasing the wealth of knowledge in the field, it helps to ensure the relevance of the targeted contributions relative to existing work. 
In this thesis, we are interested in the influence of complex situations, either complex unstructured environments or challenging weather conditions, on \gls*{lidar}-based robot navigation. This is a high-level problem involving several subtopics. Therefore, we will start our literature review with a general overview of the work on \gls*{lidar} functioning (Section~\ref{sec:literature_lidar_functionning}) and navigation in complex environments (Section~\ref{sec:literature_complex_environments}).  Subsequently, in Section~\ref{sec:literature_snow}, we will discuss research related to navigation in snowfall conditions, which is the linked to Chapter~\ref{chap:lidar_snow}. Finally, to contextualize Chapter~\ref{chap:slam}, Section~\ref{sec:literature_slam} will present papers related to feature-based place recognition and navigation in forest environments. 


\section{Overview: \gls*{lidar} functioning}
\label{sec:literature_lidar_functionning}

\gls*{lidar}s use laser to get distance, generally acquire (a collection of) multiple data point to create 2d/3d structure representation.
A general understanding of the physic behind the lidar helps understand what could possibly go wrong and how to prevent it.
More details and figures (brief overview for what interest us) in Chapter~\ref{chap:lidar_snow}, specifically Section~\ref{sec:chap_lidar_basics}.
Also, because there is multiple lidar available, choose depends on requirements (2d/3d, range, weight).


~\cite{Bosch2001}:
Review usual lidar techniques (pulsed, phase-shift and frequency modulated continuous wave), basics then limitations.
~\cite{Pomerleau2015a}:
Figure with different 3d sensors with weight and max range. small for on-board indoor, big large range for airborne


\section{Overview: Navigation in Complex Environments}
\label{sec:literature_complex_environments}

Start with a navigation definition... Which include stuff like ... Obstacle avoidance/traversability for path planning, localisation and mapping... 
Give some insight for what might be a "complex" environment (there is a gradation from completely controlled, to indoor, to outdoor, to forest etc.)

\subsubsection{Path Planning}
Geolocation, obstacles avoidance / traversability (including ground classification) = path planning

~\cite{Hussein2015}:
Geolocation of ugv in forest (horizontal position) by finding geometric match between a map of observed tree stems using onboard lidar with a map generated from the structure of thee crowns from high resolution aerial orthoimagery of the forest canopy. No need for a priori knowledge of the area surrounding the robot, only input is the geometry of detected tree stems.

\subsubsection{Localisation and Mapping + SLAM}
Start with 2d solutions. (Gmapping), but can't deal with 3D conditions or uneven bearing surface. Hector propose a 2d + imu that slighly handle uneven floor.
State that 2D solutions for slam pretty much meet our needs, they are able to yield precise results in real time\dots

~\cite{Grisetti2007}: Gmapping,
They use a particle filter to solve SLAM (grid map). Each particle carry an individual map. They take into account the movement but also the most recent observations, which decrease uncertainty. They use selective selective resampling to reduce the problem of particle depletion. They do indoor AND outdoor. 

~\cite{Kohlbrecher2011}: Hector mapping,
Focus on search and rescue. Learn map of unknown environments, occupancy grid, low computational resources. Lidar + IMU. Fast approximation of map gradients and multi resolution grids. Pretend they don't need explicit loop closure\dots


Then they start using 3D, this is more applicable to complex environments (remove some assumptions such as even ground).
Quickly explain ICP, can be used for small steps localisation, but converge in minimum and can't be used directly for localisation (for any given input scan)

~\cite{Besl1992}:
Original ICP article

~\cite{Pomerleau2015a}:
A Review of Point Cloud Registration Algorithms for Mobile Robotics\dots

~\cite{Colas2013}:
Article doing path planning in complex environments for search and rescue based on ICP for 3D terrains. 

~\cite{Pomerleau2013}:
This article present an analysis of ICP variants for real world applications. They also propose an open-source icp library that operate in real time.  

The Marulan Data Sets~\citep{Peynot2010}: state that, even if not snow, similar because of particles and could be used for assessing algorithm or do similar statistical studies for comparison. 


\section{Snowfall Conditions}
\label{sec:literature_snow}

Snowfall conditions are challenging because snowflakes cause occlusion and, because of their small size and dynamic nature, they tend to produce noise in sensors acquisitions. While it is often possible to avoid navigating in these conditions, some robots will inevitably face this situation in order to perform the tasks for which they were designed. For instance, ~\citet{Moorehead_1999_2122} aim at develop a robot to search and classify meteorites in Antartica. In the experiments, the \gls*{ugv} drives autonomously for \SI{10.3}{\kilo\meter} in different weather and terrain conditions. The navigation is based on stereo camera images and single line \gls*{lidar} scans. In this article, the authors explain that the snow-covered surfaces do not provide a lot of visual cues, which makes stereo cameras unreliable. For this reason, the robot take advantage of the \gls*{lidar}, mainly for obstacle avoidance, but often as single sensor. In return, the authors stated that part of the experiments \enquote{was performed during heavy snow which made the laser useless}, and therefore used alternative solutions. 

Another example of robots that require to handle all-weather conditions are those deployed on the battlefield. \citet{yamauchi2010fusing} uses ultra-wideband radar, stereo camera and \gls*{lidar} data for navigation. According to the authors, \enquote{[\gls*{lidar}] and stereo vision provide greater accuracy and resolution in clear weather but has difficulty with precipitation and obscurants}. The ultra-wideband radar has the ability to see through small particles such as snow, rain and fog and is therefore complementary to others sensors used. The final system achieve good results by using traditional sensor fusion, as well as a selective use of the sensors which are activated or deactivated depending on the conditions. 

\citet{sumi-arso-13}, for their part, aimed to evaluate sensors for personal care robots in natural lighting and falling snow conditions. For that matter, they built two simulators that reproduce those conditions and used three type of sensor: an active stereo sensor (Microsoft Kinect), a \gls*{lidar} (Mesa SR4000) and a passive stereo sensor (PGR Bumblebee2). This approach is interesting because it allows to control various parameters that may affect the sensor readings. By cons, information related to our research, that is to say, the impact of snow flakes on \gls*{lidar} measurements, was quite limited. In addition, the physical properties of simulated snowflakes can be different from that of real snowflakes, leading to inaccurate results for real applications.

Article by ~\citet{barnum2010analysis} is probably the most similar to the work that we will present in Chapter~\ref{chap:lidar_snow}, but applied to video rather than \gls*{lidar}s data. They explain that rain and snow are dynamic process, which cause spatial and temporal fluctuations in videos. Although the spatio-temporal changes appear to be chaotic, they predict the overall effect of these conditions on the video, in frequency space. This modeling of the effect of weather conditions on videos improve noise filtering for features extraction when compared to previous pixel-based or patch-based methods.

Finally, \citet{servomaa2002snowfall} have installed a set a of sensors for snowfall observation. The system is composed of a radar and a \gls*{lidar} that record the atmospheric profile up to \SI{6000}{\meter} and another radar along with two balances to record snowfall at ground level. Although the installation are fixed and the \gls*{lidar} is used only to evaluate transition in cloud conditions, they propose techniques for to estimate snowfall characteristics. These techniques may advantageously be used to adapt the behavior of the robot depending on weather conditions.  

%~\cite{lever2013autonomous}: ice sheet Antartica and Greenland Earth's climate history, life in extreme environments and the evolution of the cosmos, remote = expensive, ground-penetrating radar to detect crevasse of void. WOFFFF....


\section{Feature-Based Place Recognition and Forest Environments}
\label{sec:literature_slam}
Place 
\begin{itemize}
    \item Talk about features first (the idea originate from 2D images, list the most popular)
    \item Talk about features-based place recognition in general (camera-based, NDT...)
    \item Talk about tree trunk as features in forest
\end{itemize}

While we will not address this topic specifically, the algorithm used for the place recognition analysis highly depends on the chosen features.  
Bring table about features here if I do. Might want to include the part about tree features discussed in the next section. Put comparative analysis. Tackle DNN.
~\cite{Steder2011a} NARF, ~\cite{Rusu2009} FPFH, ~\cite{Yu2009} ISS, ~\cite{Tombari2010} SHOT, ~\cite{Harris1988} Harris, ~\cite{Lowe2004} SIFT, ~\cite{Bay2006} SURF

~\cite{Filipe2014}:
Comparative evalutation of 3D Keypoint Detectors in RGB-D Obeject dataset. They verify the invariance of the 3D keypoints detectors to rotaions, scales and translation.

~\cite{Magnusson2009}:
This is NDT. First in my knowledge to do loop closure based on 3D laser scans. Address the problems of greater amount of data (3D vs 2D) and the 3D rotation invariance. NDT surface representation to create feature histograms based on the surface orientation and smoothness (this compress data). Rotation invariance via alignment of dominant surface orientations.

~\cite{Song2012}:(might be good to cite original ICP or review from F. Pomerleau)
They proposes a localisation solution in forested environments. They use the largest group of approximately parallel tree trunk features to align successive scans along five dimensions (except z). Optimal transformation determined based on the axes of two tree pairs. Then they align ground by minimizing the differences of points shared in the cells. (they basically do ICP, but they pretend to remove problems such as initial transformation and noisy scene.) (This is high level engineered features, could use this for smooth transition in the text)

~\cite{Iagnemma2012}:
The method used in the previous article to extract tree truck parameters from point cloud. 3 purely geometric steps: First, the raw point clouds are segmented by utilizing the circular shape of trees, and segments are grouped into tree sections based on the principle of spatial proximity. Second, circles and axes are extracted from tree sections which are subject to loss of shape information. Third, by clustering and integrating the tree sections resulted from various space inconsistencies, straight tree trunk landmarks are finally formed for future localization. The experimental results from real forested environments are presented.

~\cite{Mcdaniel2012}:
Article that segment ground and trees that I used at the beginning of my master. (include in the path planning section)

~\cite{Lalonde2006}:
J-F Lalonde article about vegetation classification using eigenvalues/vectors. (include in the path planning section)

~\cite{Wurm2009}:
This paper present a technique that use laser remission values to detect grass like vegetation. They use a self-supervised classification vibration based. (include in the path planning section)

\begin{table}[H]
    \centering
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Type of data}  & \textbf{Keypoint/descriptor} & \textbf{Name}               & \textbf{Reference} \\
        \hline
        Image                  & Keypoint                     & Harris and Stephens corners & \cite{Harris1988}  \\
        Image                  & Both                         & SIFT                        & \cite{Lowe2004}    \\
        Image                  & Both                         & SURF                        & \cite{Bay2006}     \\
        \gls*{3d}              & Descriptor                   & FPFH                        & \cite{Rusu2009}    \\
        \gls*{3d}              & Both                         & ISS                         & \cite{Yu2009}      \\
        \gls*{3d}              & Descriptor                   & SHOT                        & \cite{Tombari2010} \\
        \gls*{3d}              & Both                         & NARF                        & \cite{Steder2011a} \\
        \bottomrule
    \end{tabular}
    \caption[Examples of popular descriptors and keypoints detectors for images and \gls*{3d} data.]{Examples of popular descriptors and keypoints detectors for images and \gls*{3d} data. Some \gls*{2d} keypoints have been adapted for \gls*{3d} such as Harris and Stephens and SIFT.}
    \label{tab:features_examples}
\end{table}
