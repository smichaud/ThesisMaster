\chapter{\chapzerotitle}
\label{chap:literature_review}


\section{Introduction}

Mobile robotics literature contains a wide variety of \gls*{lidar}-based solutions to navigation problems. This include using \gls*{2d} for indoor \gls*{slam}~\citep{Grisetti2007, Kohlbrecher2011}, geolocation in forest~\citep{Hussein2015} or detection of traversable grass-like vegetation using laser remission~\citep{Wurm2009}. As it is the case for this thesis, some studies focus their efforts on finding solutions for challenging conditions (or environments). It should be noted that there is no clear definition of what constitutes \emph{challenging} conditions, as it varies from sensor to sensor. A good understanding of the \gls*{lidar} functioning~\citep{Bosch2001} therefore helps understand how it applies to this sensor (see Section~\ref{sec:chap_lidar_basics}). The Marulan Data Sets~\citep{Peynot2010} contain good examples of such conditions, including natural outdoor environments and area with presence of smoke, dust and rain.

In this thesis, we are interested in the influence of such difficult situations, either due to complex unstructured environments or challenging weather conditions, on \gls*{lidar}-based robot navigation. In Section~\ref{sec:literature_snow}, we will discuss research related to navigation in snowfall conditions, which is the linked to Chapter~\ref{chap:lidar_snow}. Subsequently, Section~\ref{sec:literature_slam} will present papers related to feature-based place recognition and navigation in forest environments, which are more closely related to Chapter~\ref{chap:slam}. 


\section{Snowfall Conditions}
\label{sec:literature_snow}

Snowfall conditions are challenging as snowflakes cause occlusion or interference with the laser beam. Because of their small size and dynamic nature, they tend to produce a signal similar to random noise in sensors acquisitions. While it is often possible to avoid navigating in these conditions, some robots will inevitably face this situation in order to perform the tasks for which they were designed. For instance, ~\citet{Moorehead_1999_2122} aimed at developing a robot to search and classify meteorites in Antartica. In their experiments, the \gls*{ugv} droves autonomously for \SI{10.3}{\kilo\meter} in different weather and terrain conditions. The navigation was based on stereo camera images and single line \gls*{lidar} scans. In this article, the authors explain that the snow-covered surfaces did not provide a lot of visual cues, which made stereo cameras unreliable. For this reason, the robot take advantage of the \gls*{lidar}, mainly for obstacle avoidance, but often as single sensor. In return, the authors stated that part of the experiments \enquote{was performed during heavy snow which made the laser useless}, and therefore used alternative solutions. As we will see in Chapter~\ref{chap:lidar_snow}, new \gls*{lidar} technologies help reduce the impact of small particles on the readings. 

Another example of robots which need to handle all-weather conditions are those deployed on the battlefield. \citet{yamauchi2010fusing} uses ultra-wideband radar, stereo camera and \gls*{lidar} data for navigation. According to the authors, \enquote{[\gls*{lidar}] and stereo vision provide greater accuracy and resolution in clear weather but has difficulty with precipitation and obscurants}. The ultra-wideband radar has the ability to see through small particles such as snow, rain and fog and is therefore complementary to others sensors used. The final system achieve good results by using traditional sensor fusion, as well as a selective use of the sensors, which are activated or deactivated depending on the conditions. 

\citet{sumi-arso-13}, for their part, aimed at evaluating sensors for personal care robots in natural lighting and falling snow conditions. For that matter, they built two simulators that reproduced those conditions and used three type of sensor: an active stereo sensor (Microsoft Kinect), a \gls*{lidar} (Mesa SR4000) and a passive stereo sensor (PGR Bumblebee2). This approach is interesting since it allowed to control various parameters that may affect the sensor readings. However, the physical properties of simulated snowflakes can be different from that of real snowflakes, leading to inaccurate results for real applications. By contrast, as we used natural events to estimate the impact of snow flakes on \gls*{lidar} measurements, we were not able to control the environment parameters.

The work by ~\citet{barnum2010analysis} is probably the most similar to our work presented in Chapter~\ref{chap:lidar_snow}, but applied to video rather than \gls*{lidar}s data. They explain that rain and snow are dynamic process, which cause spatial and temporal fluctuations in videos. Although the spatio-temporal changes appear to be chaotic, they were able to predict the overall effect of these conditions on the video, in frequency space. This modeling of the effect of weather conditions on videos improved the noise filtering for features extraction, when compared to previous pixel-based or patch-based methods.

Finally, \citet{servomaa2002snowfall} have installed a set a of sensors for snowfall observation. The system was composed of a radar and a \gls*{lidar} that recorded the atmospheric profile up to \SI{6000}{\meter} and another radar along with two balances to record snowfall at ground level. Although the installation were fixed and the \gls*{lidar} was used only to evaluate transition in cloud conditions, they proposed techniques to estimate snowfall characteristics. Although not specifically mentioned in their paper, these techniques may advantageously be used to adapt the behavior of the robot depending on weather conditions.  

%~\cite{lever2013autonomous}: ice sheet Antartica and Greenland Earth's climate history, life in extreme environments and the evolution of the cosmos, remote = expensive, ground-penetrating radar to detect crevasse of void. WOFFFF....


\section{Feature-Based Place Recognition and Forest Environments}
\label{sec:literature_slam}

As we will see in more details in Chapter~\ref{chap:slam}, place recognition is a useful tool for mobile robot navigation. A robot can determine whether he is in previously visited place or not by comparing its current sensor acquisition with those acquired earlier. Since the relevant information density in the raw sensor data is low, data is usually converted into a representation that better capture the key information. This process is known as features extraction and consist in identifying points of interest from the sensor data (e.g. image), called feature keypoints, and create a descriptor for each keypoint. A descriptor is a vector containing values which should represent the area surrounding the keypoint robustly, even under small disturbance such as different lighting conditions or change of viewpoint. The popularity of using features representation can be attributed to the field of computer vision, especially with the introduction of SIFT~\citep{Lowe2004} and SURF~\citep{Bay2006}. This concept has since been applied to 3D data. Table~\ref{tab:features_examples} present some popular \gls*{2d} and \gls*{3d} features. 

Although the choice of features can influence the performance of place recognition, we will not address this issue directly in this thesis. Instead, we analyse the impact of some environments on the overall place recognition performance. For those further interested in this topic, a number of articles that present comparative evaluation of different features are available in the literature. For instance, \citet{Filipe2014} present an evaluation of \gls*{3d} keypoint detectors, more precisely for RGB-D objects. In this paper, they focus on the invariance of keypoints detectors to rotations, scales and translation. Similarly, \citet{Boyer2011} propose a benchmark to estimate how different algorithms perform for retrieving keypoints and descriptors when subject to different geometric transformations.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Type of data}  & \textbf{Keypoint/descriptor} & \textbf{Name}               & \textbf{Reference} \\
        \hline
        Image                  & Keypoint                     & Harris and Stephens corners & \citep{Harris1988}  \\
        Image                  & Both                         & SIFT                        & \citep{Lowe2004}    \\
        Image                  & Both                         & SURF                        & \citep{Bay2006}     \\
        \gls*{3d}              & Descriptor                   & FPFH                        & \citep{Rusu2009}    \\
        \gls*{3d}              & Both                         & ISS                         & \citep{Yu2009}      \\
        \gls*{3d}              & Descriptor                   & SHOT                        & \citep{Tombari2010} \\
        \gls*{3d}              & Both                         & NARF                        & \citep{Steder2011a} \\
        \bottomrule
    \end{tabular}
    \caption[Examples of popular descriptors and keypoints detectors for images and \gls*{3d} data.]{Examples of popular descriptors and keypoints detectors for images and \gls*{3d} data. Some \gls*{2d} keypoints have been adapted for \gls*{3d} such as Harris and Stephens and SIFT.}
    \label{tab:features_examples}
\end{table}

Several techniques have been proposed to solve the place recognition problem, but makes its appearance with approaches using cameras~\citep{Torralba2003, Ulrich2000}. The most noteworthy example is probably the work of~\citet{Cummins2008}, commonly referred as FAB-MAP. They used a probabilistic framework to recognize previously seen places and identify new places. The algorithm was able to recognize the redundant visual information that did not significantly help to distinguish places, which was use to reduce the probability of such examples to be labeled as originating from the same place. They reach recall of \SI{48}{\percent} at \SI{100}{\percent} precision for a dataset of \SI{1.9}{\kilo\meter}. The authors also proposed an enhanced version of the algorithm~\citep{Cummins2011} in which they focus on scalability primarily using the concept of inverted index. They are able to reach \SI{48}{\percent} recall at \SI{100}{\percent} precision on a \SI{70}{\kilo\meter} dataset. As we will see in Chapter~\ref{chap:slam}, place recognition is often used to detect loop closures for \gls*{slam}. In this scenario, the presence of false positives is often catastrophic, which is why they present the results for a precision of \SI{100}{\percent}.

By contrast, existing place recognition algorithms based on \gls*{3d} \gls*{lidar} data only are quite limited. To the best of our knowledge, \citet{Magnusson2009} are the first to address this problem. They used normal distribution transform (NDT) to create feature histograms based on surface orientation and smoothness to represent each scan. They also aligned scans with respect to dominant surface orientation to achieve rotation invariance. Finally they used expectation maximization to automatically determine the threshold that separated corresponding from non-corresponding scans. They achieved recall rates between \SI{22.9}{\percent} and \SI{69.6}{\percent} with false-positive rates below \SI{1.17}{\percent}, for three different datasets. More recently, ~\citet{Mack2015} proposed a similar approach for solving the place recognition problem based on \gls*{3d} lidar data. The authors indicated that they used simpler histograms and a different distance metric to achieve similar results to~\citet{Magnusson2009}. The main contributions of their algorithm is that it is simpler to implement and less computationally demanding.

The two previously mentioned algorithms used global descriptors (i.e. a single descriptor for each scan), which is often faster to process but less robust to local disturbances than approaches based on local features (i.e. a set of feature keypoints and associate descriptors for each scan). For our experiments in Chapter~\ref{chap:slam}, we will use the algorithm proposed by~\citet{Steder2011b}, which is itself an extension of their previous work in~\citep{Steder2010}. The algorithm, that will be presented in more details in Section~\ref{sec:chap_slam_algo}, use a mixture of \gls*{bow} and features matching to recognize places.

%Note that global descriptors are also used in computer vision, for instance the GIST global descriptor proposed by~\citet{oliva2001modeling}, but are more oriented toward general categorization or fast processing.

To our knowledge, none of the previously discussed algorithms have been tested in forest environments or have been developed for specific conditions. When the environment in which the robot will operate is known in advance, algorithms can be developed or fine-tuned using this prior knowledge to potentially improve performance. For instance, one could intuitively assume that in forest, tree trunks are more reliable features than foliage. ~\citet{Latulippe2013} proposed to use machine learning to automatically identify and filter local point cloud features in natural environments to be robust for scans alignment purpose. In their paper, they indeed concluded that features produced in foliage region are not reliable. Other examples of prior knowledge used for \gls*{lidar}-based navigation in forest include ~\citep{Lalonde2006}, which present a technique for segmenting data in three classes and~\citep{Mcdaniel2012}, which present a technique for segmenting ground and trees in forest. These segmented regions of data can be use to identify navigable area or be used as features for multiple tasks.

An example algorithm using this type of features, specific to the forest, is presented in~\citet{Song2012}. They proposed a localisation solution using the largest group of approximately parallel tree trunks as features to align successive scans along five dimensions (ignoring the translation relative to the gravity vector). Similarly, \citet{Miettinen2007} created a global map of trees in the form of a graph. In this graph, nodes represented tree trunks and edges represented the distance between those trunks. This representation was then used for localisation and mapping, by using best matched sub graphs.

